# 作业提交模板

```
.
├── code/                   # 所有实验代码
└── README.md               # 项目核心文档
```

## 研究目的
比较Giraph和MapReduce运行PageRank算法的差异

## 研究内容
对比分析Giraph和MapReduce在执行PageRank算法等图迭代计算任务时的差异，深入
理解Giraph所采用的BSP（Bulk Synchronous Parallel）模型的设计理念及其在图计算中的优势。
重点探讨两者在数据通信方式、任务调度机制及迭代开销等方面的不同，以及这些差异对算法性能
与可扩展性的影响。

**可拓展性差异**
- (1) 数据可拓展性：固定Worker数量，增加图规模
Metrics : 总作业时间、系统总体资源占用率
- (2) 水平可拓展性: 固定数据规模(大规模图)，增加节点数量
Metrics : 加速比 ，资源利用率 

## 实验
### 实验环境
* 硬件：集群配置，包括节点数 **(>=3)**、CPU 核数、内存大小、网络带宽、存储类型（SSD / HDD）等。

### 软件环境配置表

|**配置项目**|**详细内容**|
|---|---|
|**部署方式**|Docker 部署 (手工制作镜像)|
|**x86 镜像地址**|`docker pull frestrain/pagerank:x86`|
|**Arm 镜像地址**|`docker pull frestrain/pagerank:arm`|
|**操作系统**|Ubuntu 18.04|
|**Hadoop 版本**|3.3.6|
|**JDK 版本**|OpenJDK 11|
|**Giraph 版本**|Giraph 2.7|


### 实验负载
#### 1.数据结构（邻接表）
|**字段**|**含义**|**示例**|**作用**|
|---|---|---|---|
|**节点 ID (Node ID)**|当前页面的唯一标识|`0`|标识当前权重所属的对象|
|**PR Score**|节点的当前重要性分数|`0.0000025`|迭代计算的核心数值，随迭代更新|
|**当前节点的指向的节点**|该页面指向的所有链接|`124543, 172009...`|决定了 PR 值将分发给哪些节点|
#### 2.使用的数据集（pagerank数据）
|**文件名称**|**原始大小（字节）**|**说明大小 (MB)**|**备注**|
|---|---|---|---|
|`random_pagerank_data_100.txt`|110,056,298|约 104.96 MB|中型测试数据集|
|`random_pagerank_data_300.txt`|346,925,030|约 330.85 MB|大型数据集|
|`random_pagerank_data_500.txt`|583,623,430|约 556.59 MB|超大型数据集|
|`web-Google-PR-Init.txt`|50,598,489|约 48.25 MB|初始化/真实提取数据|
### 实验步骤
列出执行实验的关键步骤，并对关键步骤进行截图，如 MapReduce / Spark / Flink 部署成功后的进程信息、作业执行成功的信息等，**截图能够通过显示用户账号等个性化信息佐证实验的真实性**。

#### 第一阶段：环境搭建与准备
1. **基础集群部署：** 搭建Hadoop分布式集群，配置 Hadoop 的 yarn-site.xml 和 mapred-site.xml。确保其 Zookeeper 能够正常协调各 Worker。
2. **环境检查：** 环境环境验证 java -version、hadoop version 和 giraph 命令是否正常。
3. **测试:** 在单机下跑pagerank和 Giraph 自带的 SimpleShortestPathsComputation，确保计算框架链路通畅。
#### 第二阶段：算法编程实现
1. **MapReduce 版实现：**
2. **Giraph (BSP) 版实现：**
---
#### 第三阶段：数据集准备与预处理
1. **数据选取：** 准备三个不同规模的图数据集以及真实提取数据集
    - 将原始数据转换为邻接表格式（确保Mapreduce和Giraph在同一负载下)
    - 确保两套系统使用相同的输入源，以保证实验的公平性。
---
#### 第四阶段：实验运行与数据采集
1. **预实验：** 在小规模数据集上测试，验证两个版本PageRank的计算结果是否一致。
2. **正式实验：**
    - **维度 A（迭代次数）：** 固定数据集，改变迭代次数（如 10, 20, 50 次），记录执行时间。
    - **维度 B（数据规模）：** 固定迭代次数，改变节点和边数，观察作业的可扩展性。
3. **性能指标采集：**
    - **作业总时长：** 从提交到任务结束的时间。
    - **迭代间隙开销：** 记录 MapReduce 频繁读写 HDFS 产生的 I/O 耗时。
    - **网络通信量：** 记录 Giraph 在 BSP 同步点（Barrier）前后的网络流量。
---

#### 第五阶段：对比分析与结论撰写
1. **执行效率分析：** * 对比发现 Giraph 在迭代计算中由于 **数据驻留内存** 和 **无需频繁启动 Job**，其性能通常远优于 MapReduce。
2. **编程复杂度总结：**
    - 分析 Giraph “以顶点为中心”的编程模式是否比 MapReduce 的拆分逻辑更直观。
3. **BSP 模型深度剖析：**
    - 结合实验数据，解释 **超级步（Superstep）** 机制如何通过内存消息传递取代了 MapReduce 繁重的 **Shuffle-to-Disk** 过程。
4. **可扩展性讨论：**
    - 观察当数据规模超出内存容量时，Giraph 的表现是否会大幅下降（触发 Spill），从而分析其局限性。
    - 
### 实验结果与分析
使用表格和图表直观呈现结果，并解释结果背后的原因。
### 结论
总结研究的主要发现。
### 分工
尽可能详细地写出每个人的具体工作和贡献度，并按贡献度大小进行排序。
